# Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
# http://scikit-learn.org/stable/about.html#citing-scikit-learn
# Some part of this code may have been generated by modifying code from scikit-learn documentation
# ref: http://xgboost.readthedocs.io/en/latest/model.html
# ref: https://keras.io/

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegressionCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn import ensemble
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import warnings
from sklearn.metrics import accuracy_score
from sklearn.metrics import auc, roc_curve
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
import xgboost as xgb
import itertools

warnings.filterwarnings(action="ignore")

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    #print(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

df = pd.read_csv('/home/ldua/ML/upsampledSMOTEnew.csv')

# print(len(np.where(df['Attrition']==0)[0]))
Y_train = df.Attrition.values
df = df.drop('Attrition', 1)
X_train = df
test = pd.read_csv('/home/ldua/ML/Test.NoEmployeeNumber.csv')
Y_test = test.Attrition.values
X_test = test.drop('Attrition', 1)

# model = LogisticRegression()
confusion_mat = list()

decsion_tree_model = DecisionTreeClassifier(criterion='gini', max_depth=10, max_leaf_nodes=23, splitter='best')
decsion_tree_model.fit(X_train, Y_train)
y_pred_dt = decsion_tree_model.predict(X_test)
cm_dt = confusion_matrix(Y_test, y_pred_dt)
confusion_mat.append((cm_dt, "Decision Tree"))
accuracy = (cm_dt[0][0] + cm_dt[1][1]) / len(Y_test)
print("\nDecision Tree")
print(accuracy)
print(cm_dt)

extra_trees_model = ExtraTreesClassifier(random_state=2, n_estimators=200)
extra_trees_model.fit(X_train, Y_train)
y_pred_et = extra_trees_model.predict(X_test)
cm_et = confusion_matrix(Y_test, y_pred_et)
confusion_mat.append((cm_et, "Extra Trees"))
accuracy = (cm_et[0][0] + cm_et[1][1]) / len(Y_test)
print("\nExtra Trees")
print(accuracy)
print(cm_et)

random_forest_model = RandomForestClassifier(random_state=1)  # criterion='gini', max_depth=10, max_leaf_nodes=23
random_forest_model.fit(X_train, Y_train)
y_pred_rf = random_forest_model.predict(X_test)
cm_rf = confusion_matrix(Y_test, y_pred_rf)
confusion_mat.append((cm_rf, "Random Forest Classifier"))
accuracy = (cm_rf[0][0] + cm_rf[1][1]) / len(Y_test)
print("\nRandom Forest Classifier")
print(accuracy)
print(cm_rf)

gnb = GaussianNB()
gnb.fit(X_train, Y_train)
y_pred_gnb = gnb.predict(X_test)
cm_gnb = confusion_matrix(Y_test, y_pred_gnb)
confusion_mat.append((cm_gnb, "Naive Bayes Classifier"))
accuracy = (cm_gnb[0][0] + cm_gnb[1][1]) / len(Y_test)
print("\nNaive Bayes Classifier")
print(accuracy)
print(cm_gnb)

logistic_model = LogisticRegressionCV(10, random_state=1)
logistic_model.fit(X_train, Y_train)
y_pred_log = logistic_model.predict(X_test)
cm_log = confusion_matrix(Y_test, y_pred_log)
confusion_mat.append((cm_log, "Logistic Regression"))
accuracy = (cm_log[0][0] + cm_log[1][1]) / len(Y_test)
print("\nLogistic Regression")
print(accuracy)
print(cm_log)

ada_model = AdaBoostClassifier(random_state=2, n_estimators=200)
ada_model.fit(X_train, Y_train)
y_pred_ada = ada_model.predict(X_test)
cm_ada = confusion_matrix(Y_test, y_pred_ada)
confusion_mat.append((cm_ada, "Ada Boost"))
accuracy = (cm_ada[0][0] + cm_ada[1][1]) / len(Y_test)
print("\nAda Boost Classifier")
print(accuracy)
print(cm_ada)

svc = SVC(kernel='linear')  # class_weight={1:cw}
svc.fit(X_train, Y_train)
y_pred_svc = svc.predict(X_test)
cm_svc = confusion_matrix(Y_test, y_pred_svc)
confusion_mat.append((cm_svc, "Support Vector Classifier"))
accuracy = (cm_svc[0][0] + cm_svc[1][1]) / len(Y_test)
print("\nSupport Vector Classifier")
print(accuracy)
print(cm_svc)

params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2, 'learning_rate': 0.01}
gbc = ensemble.GradientBoostingClassifier(**params)
gbc.fit(X_train, Y_train)
y_pred_gbc = gbc.predict(X_test)
cm_gbc = confusion_matrix(Y_test, y_pred_gbc)
confusion_mat.append((cm_gbc, "Gradient Boosting Classifier"))
accuracy = (cm_gbc[0][0] + cm_gbc[1][1]) / len(Y_test)
print("\nGradient Boosting Classifier")
print(accuracy)
print(cm_gbc)

xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, Y_train)
y_pred_xgb = xgb_model.predict(X_test)
cm_xgb = confusion_matrix(Y_test, y_pred_xgb)
confusion_mat.append((cm_xgb, "XG Boost"))
accuracy = (cm_xgb[0][0] + cm_xgb[1][1]) / len(Y_test)
print("\nXG Boost")
print(accuracy)
print(cm_xgb)

# Neural Nets Classifier code included in separate file
y_pred_nn = np.loadtxt("/home/ldua/Neural_Nets_Prediction_Balanced", dtype=int)
cm_nn = confusion_matrix(Y_test, y_pred_nn)
confusion_mat.append((cm_nn, "Neural Nets Classifier"))
accuracy = (cm_nn[0][0] + cm_nn[1][1]) / len(Y_test)
print("\nNeural Nets")
print(accuracy)
print(cm_nn)

fprlog, tprlog, thresholds = roc_curve(Y_test, y_pred_log)
fpr2, tpr2, thresholds = roc_curve(Y_test, y_pred_ada)
fpr3, tpr3, thresholds = roc_curve(Y_test, y_pred_dt)
fpr4, tpr4, thresholds = roc_curve(Y_test, y_pred_et)
fpr5, tpr5, thresholds = roc_curve(Y_test, y_pred_rf)
fpr6, tpr6, thresholds = roc_curve(Y_test, y_pred_gnb)
fpr7, tpr7, thresholds = roc_curve(Y_test, y_pred_svc)
fpr8, tpr8, thresholds = roc_curve(Y_test, y_pred_gbc)
fpr9, tpr9, thresholds = roc_curve(Y_test, y_pred_xgb)
fpr10, tpr10, thresholds = roc_curve(Y_test, y_pred_nn)


plt.figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')
plt.rcParams.update({'font.size': 14})
plt.xlabel('FP rate', fontsize=14, color='red')
plt.ylabel('TP rate', fontsize=14, color='red')
plt.ylim([0.0, 1.0])
plt.xlim([0.0, 1.0])
plt.title('ROC curve', fontsize=14, color='red')
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k')

roc_auc = auc(fprlog, tprlog)
plt.plot(fprlog, tprlog, label='Logistic Regression: ' + str(roc_auc)[0:7])
#plt.annotate('', xy=(fprlog[1],tprlog[1]))

roc_auc = auc(fpr2, tpr2)
plt.plot(fpr2, tpr2, label='Ada boost: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr3, tpr3)
plt.plot(fpr3, tpr3, label='Decision Tree: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr4, tpr4)
plt.plot(fpr4, tpr4, label='Extra Trees: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr5, tpr5)
plt.plot(fpr5, tpr5, label='Random Forest: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr6, tpr6)
plt.plot(fpr6, tpr6, label='Gaussian Naive Bayes: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr7, tpr7)
plt.plot(fpr7, tpr7, label='Support Vector: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr8, tpr8)
plt.plot(fpr8, tpr8, label='Gradient Boosting: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr9, tpr9)
plt.plot(fpr9, tpr9, label='XGB Boost: ' + str(roc_auc)[0:7])

roc_auc = auc(fpr10, tpr10)
plt.plot(fpr10, tpr10, label='Neural Nets: ' + str(roc_auc)[0:7])

#-----Ensemble Model-----
Y_len = len(Y_test)
y_pred_ensemble = np.zeros(Y_len)
for i in range(Y_len):
    if (y_pred_rf[i] + y_pred_ada[i] + y_pred_nn[i]) >= 2:
        y_pred_ensemble[i] = 1
    else:
        y_pred_ensemble[i] = 0

cm_ensemble = confusion_matrix(Y_test, y_pred_ensemble)
#confusion_mat.append((cm_ensemble,"Ensemble Model"))
accuracy = (cm_ensemble[0][0] + cm_ensemble[1][1]) / len(Y_test)
print("\nEnsemble Model")
print(accuracy)
print(cm_ensemble)

plt.legend(fontsize=14)
plt.legend(fontsize=14)
plt.grid(True)
plt.show()
class_names = np.array(['No', 'Yes'])
for i in confusion_mat:
    plt.figure()
    plot_confusion_matrix(i[0], classes=class_names, normalize=True,
                          title='Normalized Confusion Matrix ' + i[1])
    plt.show()
