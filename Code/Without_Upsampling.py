# Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.
# http://scikit-learn.org/stable/about.html#citing-scikit-learn
# Some part of this code may have been generated by modifying code from scikit-learn documentation
# ref: http://xgboost.readthedocs.io/en/latest/model.html
# ref: https://keras.io/

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.utils import shuffle
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegressionCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn import ensemble
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import warnings
from sklearn.metrics import accuracy_score
from sklearn.metrics import auc, roc_curve
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
import xgboost as xgb
import itertools
warnings.filterwarnings(action="ignore")

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    #print(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

class_names = np.array(['No', 'Yes'])
confusion_mat=list()
train = pd.read_csv('/home/ldua/ML/Train.NoEmployeeNumber.csv')
test = pd.read_csv('/home/ldua/ML/Test.NoEmployeeNumber.csv')
Y_train=train.Attrition.values
X_train=train.drop('Attrition',1)
Y_test=test.Attrition.values
X_test=test.drop('Attrition',1)

model = LogisticRegressionCV(10,random_state=1)
model.fit(X_train, Y_train)
y_pred_log = model.predict(X_test)
cm_log = confusion_matrix(Y_test, y_pred_log)
confusion_mat.append((cm_log,"Logistic Regression"))
accuracy = (cm_log[0][0] + cm_log[1][1]) / len(Y_test)
print ("\nLogistic Regression")
print(accuracy)
print(cm_log)

model = DecisionTreeClassifier(criterion='gini', max_depth=10, max_leaf_nodes=23, splitter='best')
model.fit(X_train, Y_train)
y_pred_log_dt = model.predict(X_test)
cm_log = confusion_matrix(Y_test, y_pred_log_dt)
confusion_mat.append((cm_log,"Decision Tree Classifier"))
accuracy = (cm_log[0][0] + cm_log[1][1]) / len(Y_test)
print ("\nDecision Tree Classifier")
print(accuracy)
print(cm_log)

model = ExtraTreesClassifier()
model.fit(X_train, Y_train)
y_pred_log_et = model.predict(X_test)
cm_log = confusion_matrix(Y_test, y_pred_log_et)
confusion_mat.append((cm_log,"Extra Trees Classifier"))
accuracy = (cm_log[0][0] + cm_log[1][1]) / len(Y_test)
print ("\nExtra Trees Classifier")
print(accuracy)
print(cm_log)

random_forest_model = RandomForestClassifier(random_state=1) #criterion='gini', max_depth=10, max_leaf_nodes=23
random_forest_model.fit(X_train, Y_train)
y_pred_rf = random_forest_model.predict(X_test)
cm_rf = confusion_matrix(Y_test, y_pred_rf)
confusion_mat.append((cm_rf,"Random Forest Classifier"))
accuracy = (cm_rf[0][0] + cm_rf[1][1]) / len(Y_test)
print ("\nRandom Forest Classifier")
print(accuracy)
print(cm_rf)

gnb = GaussianNB()
gnb.fit(X_train, Y_train)
y_pred_gnb = gnb.predict(X_test)
cm_gnb = confusion_matrix(Y_test, y_pred_gnb)
confusion_mat.append((cm_gnb,"Naive Bayes Classifier"))
accuracy = (cm_gnb[0][0] + cm_gnb[1][1]) / len(Y_test)
print ("\nNaive Bayes Classifier")
print(accuracy)
print(cm_gnb)

regr = xgb.XGBClassifier()
regr.fit(X_train, Y_train)
y_pred_xgb = regr.predict(X_test)
cm_xgb = confusion_matrix(Y_test, y_pred_xgb)
confusion_mat.append((cm_xgb,"Xg Boost"))
accuracy = (cm_xgb[0][0] + cm_xgb[1][1]) / len(Y_test)
print ("\nXG Boost")
print(accuracy)
print(cm_xgb)

ada_model = AdaBoostClassifier(random_state=2, n_estimators=200)
ada_model.fit(X_train, Y_train)
y_pred_ada = ada_model.predict(X_test)
cm_ada = confusion_matrix(Y_test, y_pred_ada)
confusion_mat.append((cm_ada, "Ada Boost"))
accuracy = (cm_ada[0][0] + cm_ada[1][1]) / len(Y_test)
print("\nAda Boost Classifier")
print(accuracy)
print(cm_ada)

svc = SVC(kernel='linear')   #class_weight={1:cw}
svc.fit(X_train, Y_train)
y_pred_svc = svc.predict(X_test)
cm_svc = confusion_matrix(Y_test, y_pred_svc)
confusion_mat.append((cm_svc,"Support Vector Classifier"))
accuracy = (cm_svc[0][0] + cm_svc[1][1]) / len(Y_test)
print ("\nSupport Vector Classifier")
print(accuracy)
print(cm_svc)

params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2, 'learning_rate': 0.01}
gbc = ensemble.GradientBoostingClassifier(**params)
gbc.fit(X_train, Y_train)
y_pred_gbc = gbc.predict(X_test)
cm_gbc = confusion_matrix(Y_test, y_pred_gbc)
confusion_mat.append((cm_gbc,"Gradient Boosting Classifier"))
accuracy = (cm_gbc[0][0] + cm_gbc[1][1]) / len(Y_test)
print ("\nGradient Boosting Classifier")
print(accuracy)
print(cm_gbc)

# Neural Nets Classifier code included in separate file
y_pred_nn = np.loadtxt("/home/ldua/Neural_Nets_Prediction_Imbalanced", dtype=int)
cm_nn = confusion_matrix(Y_test, y_pred_nn)
confusion_mat.append((cm_nn, "Neural Nets Classifier"))
accuracy = (cm_nn[0][0] + cm_nn[1][1]) / len(Y_test)
print("\nNeural Nets Classifier")
print(accuracy)
print(cm_nn)

fpr_log, tpr_log, thresholds = roc_curve(Y_test, y_pred_log)
fpr_dt, tpr_dt, thresholds_dt = roc_curve(Y_test, y_pred_log_dt)
fpr_et, tpr_et, thresholds_et = roc_curve(Y_test, y_pred_log_et)
fpr_rf, tpr_rf, thresholds_rf = roc_curve(Y_test, y_pred_rf)
fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(Y_test, y_pred_gnb)
fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(Y_test, y_pred_xgb)
fpr_ada, tpr_ada, thresholds_ada = roc_curve(Y_test, y_pred_ada)
fpr_svc, tpr_svc, thresholds_svc = roc_curve(Y_test, y_pred_svc)
fpr_gbc, tpr_gbc, thresholds_gbc = roc_curve(Y_test, y_pred_gbc)
fpr_nn, tpr_nn, thresholds_nn = roc_curve(Y_test, y_pred_nn)
roc_auc = auc(fpr_log, tpr_log)
roc_auc_dt = auc(fpr_dt, tpr_dt)
roc_auc_et = auc(fpr_et, tpr_et)
roc_auc_rf = auc(fpr_rf, tpr_rf)
roc_auc_gnb = auc(fpr_gnb, tpr_gnb)
roc_auc_xgb = auc(fpr_xgb, tpr_xgb)
roc_auc_ada = auc(fpr_ada, tpr_ada)
roc_auc_svc = auc(fpr_svc, tpr_svc)
roc_auc_gbc = auc(fpr_gbc, tpr_gbc)
roc_auc_nn = auc(fpr_nn, tpr_nn)


plt.figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')
plt.rcParams.update({'font.size': 14})
plt.xlabel('FP Rate',  fontsize=14, color='red')
plt.ylabel('TP Rate',  fontsize=14, color='red')
plt.ylim([0.0, 1.0])
plt.xlim([0.0, 1.0])
plt.title('ROC Curve',  fontsize=14, color='red')
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k')
plt.plot(fpr_gnb, tpr_gnb, label='AUC Gaussian Naive Bayes: '+str(roc_auc_gnb)[0:7])
plt.plot(fpr_log, tpr_log, label='AUC Logistic Regression: '+str(roc_auc)[0:7])
plt.plot(fpr_dt, tpr_dt, label = 'AUC Decision Tree: '+str(roc_auc_dt)[0:7])
plt.plot(fpr_et, tpr_et, label='AUC Extra Trees: '+str(roc_auc_et)[0:7])
plt.plot(fpr_rf, tpr_rf, label= 'AUC Random Forest: '+str(roc_auc_rf)[0:7])
plt.plot(fpr_xgb, tpr_xgb, label='AUC XGBoost: '+str(roc_auc_xgb)[0:7])
plt.plot(fpr_ada, tpr_ada, label='AUC Ada Boost: '+str(roc_auc_ada)[0:7])
plt.plot(fpr_svc, tpr_svc, label='AUC Support Vector: '+str(roc_auc_svc)[0:7])
plt.plot(fpr_gbc, tpr_gbc,label='AUC Gradient Boosting: '+str(roc_auc_gbc)[0:7])
plt.plot(fpr_nn, tpr_nn,label='AUC Neural Nets: '+str(roc_auc_nn)[0:7])

# -----Ensemble Model-----
Y_len = len(Y_test)
y_pred_ensemble = np.zeros(Y_len)
for i in range(Y_len):
    if (y_pred_rf[i] + y_pred_log[i] + y_pred_svc[i]) >= 2:
        y_pred_ensemble[i] = 1
    else:
        y_pred_ensemble[i] = 0

cm_ensemble = confusion_matrix(Y_test, y_pred_ensemble)
# confusion_mat.append((cm_ensemble,"Ensemble Model"))
accuracy = (cm_ensemble[0][0] + cm_ensemble[1][1]) / len(Y_test)
print("\nEnsemble Model")
print(accuracy)
print(cm_ensemble)

plt.legend(fontsize=14)
plt.grid(True)
plt.show()
for i in confusion_mat:
    plt.figure()
    plot_confusion_matrix(i[0], classes=class_names, normalize=True,
                          title='Normalized Confusion Matrix: '+i[1])
    plt.show()
